{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58ad16f8-8ea0-4819-8212-b4d752f0c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the code for the Basic CNN \n",
    "# Modifying the transform to include grayscale conversion\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),        # Resize the images to 256x256 pixels\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert images to grayscale\n",
    "    transforms.ToTensor(),                # Convert the images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485], std=[0.229])  # Normalize the tensors (single channel)\n",
    "])\n",
    "\n",
    "# Re-create datasets with the updated transform\n",
    "train_dataset = datasets.ImageFolder(root='train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root='val', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root='test', transform=transform)\n",
    "\n",
    "# Re-create dataloaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Note: The rest of the training process remains the same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec6afbac-720c-4cf9-8e11-dccfce89cb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.2656592798242174\n",
      "Validation Accuracy: 93.75%\n",
      "Test Accuracy: 81.57051282051282%\n",
      "Epoch 2/5, Loss: 0.06789678160638257\n",
      "Validation Accuracy: 87.5%\n",
      "Test Accuracy: 72.91666666666667%\n",
      "Epoch 3/5, Loss: 0.04348514026296582\n",
      "Validation Accuracy: 87.5%\n",
      "Test Accuracy: 73.87820512820512%\n",
      "Epoch 4/5, Loss: 0.020065413492637477\n",
      "Validation Accuracy: 93.75%\n",
      "Test Accuracy: 76.28205128205128%\n",
      "Epoch 5/5, Loss: 0.014459754411425128\n",
      "Validation Accuracy: 81.25%\n",
      "Test Accuracy: 72.59615384615384%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# CNN Model Definition for grayscale images\n",
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)  # Accepts 1-channel input\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 64 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = BasicCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Assuming you are using a GPU if available\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.to(device)\n",
    "\n",
    "\n",
    "# Function for training\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=5):\n",
    "    num_epochs = epochs\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Validation Accuracy: {100 * correct / total}%')\n",
    "\n",
    "    # Test the model\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "    \n",
    "        print(f'Test Accuracy: {100 * correct / total}%')\n",
    "\n",
    "# Train the model (assuming train_loader and val_loader are already defined)\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11451f8f-be7f-4a1a-9dd9-1e66e465d653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.19290123738211357\n",
      "Validation Accuracy: 87.5%\n",
      "Test Accuracy: 77.08333333333333%\n",
      "Epoch 2/10, Loss: 0.06474723963372615\n",
      "Validation Accuracy: 100.0%\n",
      "Test Accuracy: 75.64102564102564%\n",
      "Epoch 3/10, Loss: 0.03640209469748547\n",
      "Validation Accuracy: 93.75%\n",
      "Test Accuracy: 76.12179487179488%\n",
      "Epoch 4/10, Loss: 0.017129605028123388\n",
      "Validation Accuracy: 93.75%\n",
      "Test Accuracy: 75.32051282051282%\n",
      "Epoch 5/10, Loss: 0.005809777389461548\n",
      "Validation Accuracy: 93.75%\n",
      "Test Accuracy: 73.3974358974359%\n",
      "Epoch 6/10, Loss: 0.001009501473053055\n",
      "Validation Accuracy: 93.75%\n",
      "Test Accuracy: 73.23717948717949%\n",
      "Epoch 7/10, Loss: 0.00020781238604537132\n",
      "Validation Accuracy: 87.5%\n",
      "Test Accuracy: 71.9551282051282%\n",
      "Epoch 8/10, Loss: 0.00011850572800358908\n",
      "Validation Accuracy: 87.5%\n",
      "Test Accuracy: 72.27564102564102%\n",
      "Epoch 9/10, Loss: 7.870541575546854e-05\n",
      "Validation Accuracy: 87.5%\n",
      "Test Accuracy: 72.27564102564102%\n",
      "Epoch 10/10, Loss: 5.903387805963497e-05\n",
      "Validation Accuracy: 87.5%\n",
      "Test Accuracy: 72.59615384615384%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# CNN Model Definition for grayscale images\n",
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)  # Accepts 1-channel input\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 64 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = BasicCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Assuming you are using a GPU if available\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.to(device)\n",
    "\n",
    "\n",
    "# Function for training\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10):\n",
    "    num_epochs = epochs\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Validation Accuracy: {100 * correct / total}%')\n",
    "\n",
    "    # Test the model\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "    \n",
    "        print(f'Test Accuracy: {100 * correct / total}%')\n",
    "\n",
    "# Train the model (assuming train_loader and val_loader are already defined)\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "872d224d-777e-41e4-92cd-21c8a529ae7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.34061336271647663\n",
      "Validation Accuracy: 87.5%\n",
      "Test Accuracy: 74.67948717948718%\n",
      "Epoch 2/20, Loss: 0.07293965258104212\n",
      "Validation Accuracy: 100.0%\n",
      "Test Accuracy: 82.37179487179488%\n",
      "Epoch 3/20, Loss: 0.03780929441528075\n",
      "Validation Accuracy: 87.5%\n",
      "Test Accuracy: 77.08333333333333%\n",
      "Epoch 4/20, Loss: 0.019738213354121733\n",
      "Validation Accuracy: 75.0%\n",
      "Test Accuracy: 70.3525641025641%\n",
      "Epoch 5/20, Loss: 0.01906484283023979\n",
      "Validation Accuracy: 87.5%\n",
      "Test Accuracy: 73.3974358974359%\n",
      "Epoch 6/20, Loss: 0.0036249541123758323\n",
      "Validation Accuracy: 81.25%\n",
      "Test Accuracy: 75.0%\n",
      "Epoch 7/20, Loss: 0.000788282469153954\n",
      "Validation Accuracy: 81.25%\n",
      "Test Accuracy: 75.0%\n",
      "Epoch 8/20, Loss: 0.00019697701913818117\n",
      "Validation Accuracy: 81.25%\n",
      "Test Accuracy: 74.51923076923077%\n",
      "Epoch 9/20, Loss: 0.00011921529111800263\n",
      "Validation Accuracy: 81.25%\n",
      "Test Accuracy: 74.35897435897436%\n",
      "Epoch 10/20, Loss: 8.58302283887201e-05\n",
      "Validation Accuracy: 81.25%\n",
      "Test Accuracy: 74.83974358974359%\n",
      "Epoch 11/20, Loss: 6.538513674440987e-05\n",
      "Validation Accuracy: 81.25%\n",
      "Test Accuracy: 75.16025641025641%\n",
      "Epoch 12/20, Loss: 5.242542954542116e-05\n",
      "Validation Accuracy: 81.25%\n",
      "Test Accuracy: 75.0%\n",
      "Epoch 13/20, Loss: 4.0818586055425795e-05\n",
      "Validation Accuracy: 81.25%\n",
      "Test Accuracy: 72.75641025641026%\n",
      "Epoch 14/20, Loss: 3.520090472841903e-05\n",
      "Validation Accuracy: 81.25%\n",
      "Test Accuracy: 74.67948717948718%\n",
      "Epoch 15/20, Loss: 2.8407116038790943e-05\n",
      "Validation Accuracy: 81.25%\n",
      "Test Accuracy: 75.0%\n",
      "Epoch 16/20, Loss: 2.3550846681887852e-05\n",
      "Validation Accuracy: 81.25%\n",
      "Test Accuracy: 74.67948717948718%\n",
      "Epoch 17/20, Loss: 1.9662309089624425e-05\n",
      "Validation Accuracy: 81.25%\n",
      "Test Accuracy: 74.83974358974359%\n",
      "Epoch 18/20, Loss: 1.6508088378026043e-05\n",
      "Validation Accuracy: 81.25%\n",
      "Test Accuracy: 74.51923076923077%\n",
      "Epoch 19/20, Loss: 1.3919946443595208e-05\n",
      "Validation Accuracy: 81.25%\n",
      "Test Accuracy: 74.35897435897436%\n",
      "Epoch 20/20, Loss: 1.2344789193361502e-05\n",
      "Validation Accuracy: 81.25%\n",
      "Test Accuracy: 74.67948717948718%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# CNN Model Definition for grayscale images\n",
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)  # Accepts 1-channel input\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 64 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = BasicCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Assuming you are using a GPU if available\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.to(device)\n",
    "\n",
    "\n",
    "# Function for training\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=20):\n",
    "    num_epochs = epochs\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Validation Accuracy: {100 * correct / total}%')\n",
    "\n",
    "    # Test the model\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "    \n",
    "        print(f'Test Accuracy: {100 * correct / total}%')\n",
    "\n",
    "# Train the model (assuming train_loader and val_loader are already defined)\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4600b7d8-4e73-4c45-be76-f78f54fdc341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.2977103197810017\n",
      "Validation Accuracy: 75.0%\n",
      "Test Accuracy: 81.25%\n",
      "Epoch 2/20, Loss: 0.11053484615989806\n",
      "Validation Accuracy: 68.75%\n",
      "Test Accuracy: 76.28205128205128%\n",
      "Epoch 3/20, Loss: 0.098603746913762\n",
      "Validation Accuracy: 62.5%\n",
      "Test Accuracy: 80.60897435897436%\n",
      "Epoch 4/20, Loss: 0.08681882552708288\n",
      "Validation Accuracy: 50.0%\n",
      "Test Accuracy: 78.36538461538461%\n",
      "Epoch 5/20, Loss: 0.08807716566349374\n",
      "Validation Accuracy: 56.25%\n",
      "Test Accuracy: 71.47435897435898%\n",
      "Epoch 6/20, Loss: 0.07501501939116628\n",
      "Validation Accuracy: 68.75%\n",
      "Test Accuracy: 76.12179487179488%\n",
      "Epoch 7/20, Loss: 0.06873277269937297\n",
      "Validation Accuracy: 56.25%\n",
      "Test Accuracy: 68.75%\n",
      "Epoch 8/20, Loss: 0.0667259337686208\n",
      "Validation Accuracy: 62.5%\n",
      "Test Accuracy: 72.59615384615384%\n",
      "Epoch 9/20, Loss: 0.06381468534001086\n",
      "Validation Accuracy: 62.5%\n",
      "Test Accuracy: 77.40384615384616%\n",
      "Epoch 10/20, Loss: 0.0505740341699705\n",
      "Validation Accuracy: 68.75%\n",
      "Test Accuracy: 78.84615384615384%\n",
      "Epoch 11/20, Loss: 0.0567695811402304\n",
      "Validation Accuracy: 68.75%\n",
      "Test Accuracy: 76.6025641025641%\n",
      "Epoch 12/20, Loss: 0.047802873423603\n",
      "Validation Accuracy: 75.0%\n",
      "Test Accuracy: 73.23717948717949%\n",
      "Epoch 13/20, Loss: 0.044720288723895746\n",
      "Validation Accuracy: 81.25%\n",
      "Test Accuracy: 80.60897435897436%\n",
      "Epoch 14/20, Loss: 0.04871528180447504\n",
      "Validation Accuracy: 75.0%\n",
      "Test Accuracy: 79.6474358974359%\n",
      "Epoch 15/20, Loss: 0.0426431347948479\n",
      "Validation Accuracy: 68.75%\n",
      "Test Accuracy: 74.67948717948718%\n",
      "Epoch 16/20, Loss: 0.03792616338154838\n",
      "Validation Accuracy: 68.75%\n",
      "Test Accuracy: 73.87820512820512%\n",
      "Epoch 17/20, Loss: 0.04037327717981113\n",
      "Validation Accuracy: 68.75%\n",
      "Test Accuracy: 76.76282051282051%\n",
      "Epoch 18/20, Loss: 0.03596452993742173\n",
      "Validation Accuracy: 68.75%\n",
      "Test Accuracy: 74.83974358974359%\n",
      "Epoch 19/20, Loss: 0.02594850248003402\n",
      "Validation Accuracy: 75.0%\n",
      "Test Accuracy: 80.76923076923077%\n",
      "Epoch 20/20, Loss: 0.030690683019090375\n",
      "Validation Accuracy: 62.5%\n",
      "Test Accuracy: 74.35897435897436%\n"
     ]
    }
   ],
   "source": [
    "#Used the result from this one\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert images to grayscale\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the images horizontally\n",
    "    transforms.RandomRotation(10),      # Randomly rotate the images by 10 degrees\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "])\n",
    "\n",
    "# Validation and Test transform does not need augmentation, only resizing and normalization\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "])\n",
    "\n",
    "\n",
    "# Apply the updated transforms to datasets\n",
    "train_dataset = datasets.ImageFolder(root='train', transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder(root='val', transform=val_test_transform)\n",
    "test_dataset = datasets.ImageFolder(root='test', transform=val_test_transform)\n",
    "\n",
    "# Create dataloaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# CNN Model Definition for grayscale images\n",
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)  # Accepts 1-channel input\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 64 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = BasicCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Assuming you are using a GPU if available\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.to(device)\n",
    "\n",
    "\n",
    "# Function for training\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=20):\n",
    "    num_epochs = epochs\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Validation Accuracy: {100 * correct / total}%')\n",
    "\n",
    "    # Test the model\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "    \n",
    "        print(f'Test Accuracy: {100 * correct / total}%')\n",
    "\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b60a93e-3e70-4dc2-9928-5ac21fd0fb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old junk code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe20c32e-da48-466e-9352-d5b47cfce8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7bf940ac-a5ea-4342-9f67-37d3d85bfe14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.24916522739100858\n",
      "Accuracy on training set: 98.02530674846626%\n",
      "Accuracy on validation set: 56.25%\n",
      "Epoch 2, Loss: 0.057518845144087914\n",
      "Accuracy on training set: 99.00306748466258%\n",
      "Accuracy on validation set: 62.5%\n",
      "Epoch 3, Loss: 0.030602085344461964\n",
      "Accuracy on training set: 99.63573619631902%\n",
      "Accuracy on validation set: 81.25%\n",
      "Epoch 4, Loss: 0.00798086799472996\n",
      "Accuracy on training set: 99.75076687116564%\n",
      "Accuracy on validation set: 81.25%\n",
      "Epoch 5, Loss: 0.009984408150836998\n",
      "Accuracy on training set: 99.94248466257669%\n",
      "Accuracy on validation set: 93.75%\n",
      "Epoch 6, Loss: 0.004468532831713205\n",
      "Accuracy on training set: 99.02223926380368%\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 7, Loss: 0.012766865284874553\n",
      "Accuracy on training set: 99.88496932515338%\n",
      "Accuracy on validation set: 75.0%\n",
      "Epoch 8, Loss: 0.0060329802789591625\n",
      "Accuracy on training set: 99.84662576687117%\n",
      "Accuracy on validation set: 81.25%\n",
      "Epoch 9, Loss: 0.0037036283397913847\n",
      "Accuracy on training set: 100.0%\n",
      "Accuracy on validation set: 75.0%\n",
      "Epoch 10, Loss: 0.0002514705875482051\n",
      "Accuracy on training set: 100.0%\n",
      "Accuracy on validation set: 81.25%\n",
      "Accuracy on the test set: 74.52%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#This is the code for the Basic CNN \n",
    "# Modifying the transform to include grayscale conversion\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),        # Resize the images to 256x256 pixels\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert images to grayscale\n",
    "    transforms.ToTensor(),                # Convert the images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485], std=[0.229])  # Normalize the tensors (single channel)\n",
    "])\n",
    "\n",
    "# Re-create datasets with the updated transform\n",
    "train_dataset = datasets.ImageFolder(root='train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root='val', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root='test', transform=transform)\n",
    "\n",
    "# Re-create dataloaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Note: The rest of the training process remains the same.\n",
    "\n",
    "\n",
    "# CNN Model Definition for grayscale images\n",
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)  # Accepts 1-channel input\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 64 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = BasicCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Function for training\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "        # Training phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in train_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Accuracy on training set: {100 * correct / total}%\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Accuracy on validation set: {100 * correct / total}%\")\n",
    "\n",
    "# Train the model (assuming train_loader and val_loader are already defined)\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10)\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients for testing\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on the test set: {accuracy:.2f}%')\n",
    "\n",
    "# Run the test function\n",
    "test_model(model, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53e30cc5-3b9b-4561-aa6b-a2d2740f4920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.43002113485080334\n",
      "Accuracy on training set: 97.14340490797547%\n",
      "Accuracy on validation set: 81.25%\n",
      "Epoch 2, Loss: 0.08348109828811025\n",
      "Accuracy on training set: 98.06365030674847%\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 3, Loss: 0.07403142692555496\n",
      "Accuracy on training set: 97.25843558282209%\n",
      "Accuracy on validation set: 62.5%\n",
      "Epoch 4, Loss: 0.04992986581305419\n",
      "Accuracy on training set: 98.92638036809817%\n",
      "Accuracy on validation set: 93.75%\n",
      "Epoch 5, Loss: 0.034136573527475386\n",
      "Accuracy on training set: 99.73159509202453%\n",
      "Accuracy on validation set: 93.75%\n",
      "Accuracy on the test set: 73.88%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#This is the code for the Basic CNN \n",
    "# Modifying the transform to include grayscale conversion\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),        # Resize the images to 256x256 pixels\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert images to grayscale\n",
    "    transforms.ToTensor(),                # Convert the images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485], std=[0.229])  # Normalize the tensors (single channel)\n",
    "])\n",
    "\n",
    "# Re-create datasets with the updated transform\n",
    "train_dataset = datasets.ImageFolder(root='train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root='val', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root='test', transform=transform)\n",
    "\n",
    "# Re-create dataloaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Note: The rest of the training process remains the same.\n",
    "\n",
    "\n",
    "# CNN Model Definition for grayscale images\n",
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)  # Accepts 1-channel input\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 64 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = BasicCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Function for training\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "        # Training phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in train_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Accuracy on training set: {100 * correct / total}%\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Accuracy on validation set: {100 * correct / total}%\")\n",
    "\n",
    "# Train the model (assuming train_loader and val_loader are already defined)\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, epochs=5)\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients for testing\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on the test set: {accuracy:.2f}%')\n",
    "\n",
    "# Run the test function\n",
    "test_model(model, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40ce2e5d-d5d1-4867-9c0e-29813069c160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.33051465912807754\n",
      "Accuracy on training set: 97.48849693251533%\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 2, Loss: 0.07532548667303075\n",
      "Accuracy on training set: 98.40874233128834%\n",
      "Accuracy on validation set: 75.0%\n",
      "Epoch 3, Loss: 0.047235667022399355\n",
      "Accuracy on training set: 98.79217791411043%\n",
      "Accuracy on validation set: 75.0%\n",
      "Epoch 4, Loss: 0.03333959418443243\n",
      "Accuracy on training set: 99.59739263803681%\n",
      "Accuracy on validation set: 81.25%\n",
      "Epoch 5, Loss: 0.01562980540513498\n",
      "Accuracy on training set: 99.82745398773007%\n",
      "Accuracy on validation set: 93.75%\n",
      "Epoch 6, Loss: 0.008384940226016312\n",
      "Accuracy on training set: 99.9808282208589%\n",
      "Accuracy on validation set: 93.75%\n",
      "Epoch 7, Loss: 0.005358761295312232\n",
      "Accuracy on training set: 99.9808282208589%\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 8, Loss: 0.0015663489384566947\n",
      "Accuracy on training set: 100.0%\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 9, Loss: 0.00023182243666844353\n",
      "Accuracy on training set: 100.0%\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 10, Loss: 0.00014076752842791872\n",
      "Accuracy on training set: 100.0%\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 11, Loss: 8.9926800535104e-05\n",
      "Accuracy on training set: 100.0%\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 12, Loss: 6.913285515187778e-05\n",
      "Accuracy on training set: 100.0%\n",
      "Accuracy on validation set: 93.75%\n",
      "Epoch 13, Loss: 5.4993775093263376e-05\n",
      "Accuracy on training set: 100.0%\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 14, Loss: 4.550932956974214e-05\n",
      "Accuracy on training set: 100.0%\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 15, Loss: 3.789768045404222e-05\n",
      "Accuracy on training set: 100.0%\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 16, Loss: 3.237689423639755e-05\n",
      "Accuracy on training set: 100.0%\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 17, Loss: 2.802152954045861e-05\n",
      "Accuracy on training set: 100.0%\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 18, Loss: 2.4113074631308743e-05\n",
      "Accuracy on training set: 100.0%\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 19, Loss: 2.070234884924218e-05\n",
      "Accuracy on training set: 100.0%\n",
      "Accuracy on validation set: 93.75%\n",
      "Epoch 20, Loss: 1.82241418360949e-05\n",
      "Accuracy on training set: 100.0%\n",
      "Accuracy on validation set: 93.75%\n",
      "Accuracy on the test set: 72.92%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#This is the code for the Basic CNN \n",
    "# Modifying the transform to include grayscale conversion\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),        # Resize the images to 256x256 pixels\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert images to grayscale\n",
    "    transforms.ToTensor(),                # Convert the images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485], std=[0.229])  # Normalize the tensors (single channel)\n",
    "])\n",
    "\n",
    "# Re-create datasets with the updated transform\n",
    "train_dataset = datasets.ImageFolder(root='train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root='val', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root='test', transform=transform)\n",
    "\n",
    "# Re-create dataloaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Note: The rest of the training process remains the same.\n",
    "\n",
    "\n",
    "# CNN Model Definition for grayscale images\n",
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)  # Accepts 1-channel input\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 64 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = BasicCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Function for training\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "        # Training phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in train_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Accuracy on training set: {100 * correct / total}%\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Accuracy on validation set: {100 * correct / total}%\")\n",
    "\n",
    "# Train the model (assuming train_loader and val_loader are already defined)\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, epochs=20)\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients for testing\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on the test set: {accuracy:.2f}%')\n",
    "\n",
    "# Run the test function\n",
    "test_model(model, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9fd992d-8a7d-45f0-b42e-66378f2d9028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.516297682754101\n",
      "Accuracy on training set: 93.63496932515338%\n",
      "Accuracy on validation set: 62.5%\n",
      "Epoch 2, Loss: 0.18668425268282188\n",
      "Accuracy on training set: 93.80751533742331%\n",
      "Accuracy on validation set: 62.5%\n",
      "Epoch 3, Loss: 0.14801314691413034\n",
      "Accuracy on training set: 96.64493865030674%\n",
      "Accuracy on validation set: 68.75%\n",
      "Epoch 4, Loss: 0.09196361920659207\n",
      "Accuracy on training set: 99.06058282208589%\n",
      "Accuracy on validation set: 68.75%\n",
      "Epoch 5, Loss: 0.05700145080539918\n",
      "Accuracy on training set: 96.74079754601227%\n",
      "Accuracy on validation set: 68.75%\n",
      "Accuracy on the test set: 80.93%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#This is the code for the Basic CNN \n",
    "# Modifying the transform to include grayscale conversion\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),        # Resize the images to 256x256 pixels\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert images to grayscale\n",
    "    transforms.ToTensor(),                # Convert the images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485], std=[0.229])  # Normalize the tensors (single channel)\n",
    "])\n",
    "\n",
    "# Re-create datasets with the updated transform\n",
    "train_dataset = datasets.ImageFolder(root='train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root='val', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root='test', transform=transform)\n",
    "\n",
    "# Re-create dataloaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Note: The rest of the training process remains the same.\n",
    "\n",
    "\n",
    "# CNN Model Definition for grayscale images\n",
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)  # Accepts 1-channel input\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 64 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = BasicCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Function for training\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "        # Training phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in train_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Accuracy on training set: {100 * correct / total}%\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Accuracy on validation set: {100 * correct / total}%\")\n",
    "\n",
    "# Train the model (assuming train_loader and val_loader are already defined)\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, epochs=5)\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients for testing\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on the test set: {accuracy:.2f}%')\n",
    "\n",
    "# Run the test function\n",
    "test_model(model, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c828d223-712e-42e5-bd3e-602e5cb3aa68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.393149832275016\n",
      "Accuracy on training set: 74.29064417177914%\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 2, Loss: 0.5707801304346213\n",
      "Accuracy on training set: 74.29064417177914%\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 3, Loss: 0.5707237422466278\n",
      "Accuracy on training set: 74.29064417177914%\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 4, Loss: 0.5712635888278119\n",
      "Accuracy on training set: 74.29064417177914%\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 5, Loss: 0.5708554637578368\n",
      "Accuracy on training set: 74.29064417177914%\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 6, Loss: 0.5715895739435418\n",
      "Accuracy on training set: 74.29064417177914%\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 7, Loss: 0.570072204971606\n",
      "Accuracy on training set: 74.29064417177914%\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 8, Loss: 0.5714490378926869\n",
      "Accuracy on training set: 74.29064417177914%\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 9, Loss: 0.5712461817118287\n",
      "Accuracy on training set: 74.29064417177914%\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 10, Loss: 0.570421174808514\n",
      "Accuracy on training set: 74.29064417177914%\n",
      "Accuracy on validation set: 50.0%\n",
      "Accuracy on the test set: 62.50%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#This is the code for the Basic CNN \n",
    "# Modifying the transform to include grayscale conversion\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),        # Resize the images to 256x256 pixels\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert images to grayscale\n",
    "    transforms.ToTensor(),                # Convert the images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485], std=[0.229])  # Normalize the tensors (single channel)\n",
    "])\n",
    "\n",
    "# Re-create datasets with the updated transform\n",
    "train_dataset = datasets.ImageFolder(root='train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root='val', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root='test', transform=transform)\n",
    "\n",
    "# Re-create dataloaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Note: The rest of the training process remains the same.\n",
    "\n",
    "\n",
    "# CNN Model Definition for grayscale images\n",
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)  # Accepts 1-channel input\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 64 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = BasicCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Function for training\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "        # Training phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in train_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Accuracy on training set: {100 * correct / total}%\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Accuracy on validation set: {100 * correct / total}%\")\n",
    "\n",
    "# Train the model (assuming train_loader and val_loader are already defined)\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10)\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients for testing\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on the test set: {accuracy:.2f}%')\n",
    "\n",
    "# Run the test function\n",
    "test_model(model, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d143b7f9-0146-49c5-9214-52ce62d0858c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.92226667392519\n",
      "Accuracy on training set: 95.36042944785277%\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 2, Loss: 0.10986087884913924\n",
      "Accuracy on training set: 96.31901840490798%\n",
      "Accuracy on validation set: 75.0%\n",
      "Epoch 3, Loss: 0.06976008879169365\n",
      "Accuracy on training set: 97.85276073619632%\n",
      "Accuracy on validation set: 68.75%\n",
      "Epoch 4, Loss: 0.06907780927411664\n",
      "Accuracy on training set: 98.71549079754601%\n",
      "Accuracy on validation set: 93.75%\n",
      "Epoch 5, Loss: 0.05272214518579836\n",
      "Accuracy on training set: 98.38957055214723%\n",
      "Accuracy on validation set: 81.25%\n",
      "Epoch 6, Loss: 0.034098942177172444\n",
      "Accuracy on training set: 99.36733128834356%\n",
      "Accuracy on validation set: 81.25%\n",
      "Epoch 7, Loss: 0.029022306360163657\n",
      "Accuracy on training set: 99.50153374233129%\n",
      "Accuracy on validation set: 81.25%\n",
      "Epoch 8, Loss: 0.0192204802720724\n",
      "Accuracy on training set: 99.82745398773007%\n",
      "Accuracy on validation set: 93.75%\n",
      "Epoch 9, Loss: 0.10503896235839119\n",
      "Accuracy on training set: 98.1978527607362%\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 10, Loss: 0.07445493979369312\n",
      "Accuracy on training set: 99.21395705521472%\n",
      "Accuracy on validation set: 93.75%\n",
      "Epoch 11, Loss: 0.0314394185783932\n",
      "Accuracy on training set: 97.92944785276073%\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 12, Loss: 0.016371030612119697\n",
      "Accuracy on training set: 99.82745398773007%\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 13, Loss: 0.007464168752028338\n",
      "Accuracy on training set: 99.84662576687117%\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 14, Loss: 0.01711232312994228\n",
      "Accuracy on training set: 99.76993865030674%\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 15, Loss: 0.014860124070664433\n",
      "Accuracy on training set: 99.5398773006135%\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 16, Loss: 0.012944737237804869\n",
      "Accuracy on training set: 99.63573619631902%\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 17, Loss: 0.028538228439328208\n",
      "Accuracy on training set: 99.32898773006134%\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 18, Loss: 0.03155630312959186\n",
      "Accuracy on training set: 99.5398773006135%\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 19, Loss: 0.02015062501253695\n",
      "Accuracy on training set: 99.30981595092024%\n",
      "Accuracy on validation set: 75.0%\n",
      "Epoch 20, Loss: 0.07610021225071179\n",
      "Accuracy on training set: 99.48236196319019%\n",
      "Accuracy on validation set: 81.25%\n",
      "Accuracy on the test set: 75.32%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#This is the code for the Basic CNN \n",
    "# Modifying the transform to include grayscale conversion\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),        # Resize the images to 256x256 pixels\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert images to grayscale\n",
    "    transforms.ToTensor(),                # Convert the images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485], std=[0.229])  # Normalize the tensors (single channel)\n",
    "])\n",
    "\n",
    "# Re-create datasets with the updated transform\n",
    "train_dataset = datasets.ImageFolder(root='train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root='val', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root='test', transform=transform)\n",
    "\n",
    "# Re-create dataloaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Note: The rest of the training process remains the same.\n",
    "\n",
    "\n",
    "# CNN Model Definition for grayscale images\n",
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)  # Accepts 1-channel input\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 64 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = BasicCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Function for training\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "        # Training phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in train_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Accuracy on training set: {100 * correct / total}%\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Accuracy on validation set: {100 * correct / total}%\")\n",
    "\n",
    "# Train the model (assuming train_loader and val_loader are already defined)\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, epochs=20)\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients for testing\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on the test set: {accuracy:.2f}%')\n",
    "\n",
    "# Run the test function\n",
    "test_model(model, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4579701-a143-49c7-a85f-39a3e154d003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 75.32%\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients for testing\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on the test set: {accuracy:.2f}%')\n",
    "\n",
    "# Run the test function\n",
    "test_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9e9d5ee-504a-468a-9c20-86ee589d03f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.43210358728616105\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 2, Loss: 0.29746604814436406\n",
      "Accuracy on validation set: 93.75%\n",
      "Epoch 3, Loss: 0.2239149840374344\n",
      "Accuracy on validation set: 93.75%\n",
      "Epoch 4, Loss: 0.21675027567382013\n",
      "Accuracy on validation set: 68.75%\n",
      "Epoch 5, Loss: 0.12354396548438894\n",
      "Accuracy on validation set: 87.5%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize the images to 256x256 pixels\n",
    "    transforms.ToTensor(),          # Convert the images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the tensors\n",
    "])\n",
    "\n",
    "# Create datasets for training, validation, and test sets\n",
    "train_dataset = datasets.ImageFolder(root='train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root='val', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root='test', transform=transform)\n",
    "\n",
    "# Create dataloaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# These loaders can now be used to iterate over the respective datasets\n",
    "# Example usage:\n",
    "# for images, labels in train_loader:\n",
    "#     # Training process here\n",
    "\n",
    "\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze the feature layers (optional, if you want to fine-tune only the classifier)\n",
    "for param in vgg16.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the classifier for binary classification\n",
    "vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, 2)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vgg16.classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Function for training\n",
    "def train_model_vgg16(model, criterion, optimizer, train_loader, val_loader, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}, Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Example of how to call the train_model_vgg16 function\n",
    "# train_model_vgg16(vgg16, criterion, optimizer, train_loader, val_loader, epochs=5)\n",
    "\n",
    "# Train the model\n",
    "train_model(vgg16, criterion, optimizer, train_loader, val_loader, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67fe44df-c4c7-4746-ad57-bd3da177c481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 77.88%\n"
     ]
    }
   ],
   "source": [
    "test_model(vgg16, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbc0beb1-3e69-4865-915a-4de877b44a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.45553880422567145\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 2, Loss: 0.2144895821750284\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 3, Loss: 0.20211940694715022\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 4, Loss: 0.24318567452352652\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 5, Loss: 0.14384630067974652\n",
      "Accuracy on validation set: 93.75%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize the images to 256x256 pixels\n",
    "    transforms.ToTensor(),          # Convert the images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the tensors\n",
    "])\n",
    "\n",
    "# Create datasets for training, validation, and test sets\n",
    "train_dataset = datasets.ImageFolder(root='train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root='val', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root='test', transform=transform)\n",
    "\n",
    "# Create dataloaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# These loaders can now be used to iterate over the respective datasets\n",
    "# Example usage:\n",
    "# for images, labels in train_loader:\n",
    "#     # Training process here\n",
    "\n",
    "\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze the feature layers (optional, if you want to fine-tune only the classifier)\n",
    "for param in vgg16.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the classifier for binary classification\n",
    "vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, 2)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vgg16.classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Function for training\n",
    "def train_model_vgg16(model, criterion, optimizer, train_loader, val_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}, Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Example of how to call the train_model_vgg16 function\n",
    "# train_model_vgg16(vgg16, criterion, optimizer, train_loader, val_loader, epochs=5)\n",
    "\n",
    "# Train the model\n",
    "train_model(vgg16, criterion, optimizer, train_loader, val_loader, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02580452-0e56-4695-b267-f73301091120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 78.69%\n"
     ]
    }
   ],
   "source": [
    "test_model(vgg16, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84dadcf8-0b7f-4b65-a514-1a4fad69c34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4207876595613077\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 2, Loss: 0.22601135187189003\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 3, Loss: 0.19447154788474527\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 4, Loss: 0.15334871824506083\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 5, Loss: 0.09843001351075781\n",
      "Accuracy on validation set: 81.25%\n",
      "Epoch 6, Loss: 0.13009645920704097\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 7, Loss: 0.0685771342740174\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 8, Loss: 0.20909153290814467\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 9, Loss: 0.08254279886346126\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 10, Loss: 0.1612110691275936\n",
      "Accuracy on validation set: 93.75%\n",
      "Accuracy on the test set: 71.63%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize the images to 256x256 pixels\n",
    "    transforms.ToTensor(),          # Convert the images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the tensors\n",
    "])\n",
    "\n",
    "# Create datasets for training, validation, and test sets\n",
    "train_dataset = datasets.ImageFolder(root='train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root='val', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root='test', transform=transform)\n",
    "\n",
    "# Create dataloaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# These loaders can now be used to iterate over the respective datasets\n",
    "# Example usage:\n",
    "# for images, labels in train_loader:\n",
    "#     # Training process here\n",
    "\n",
    "\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze the feature layers (optional, if you want to fine-tune only the classifier)\n",
    "for param in vgg16.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the classifier for binary classification\n",
    "vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, 2)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vgg16.classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Function for training\n",
    "def train_model_vgg16(model, criterion, optimizer, train_loader, val_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}, Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Example of how to call the train_model_vgg16 function\n",
    "# train_model_vgg16(vgg16, criterion, optimizer, train_loader, val_loader, epochs=5)\n",
    "\n",
    "# Train the model\n",
    "train_model(vgg16, criterion, optimizer, train_loader, val_loader, epochs=10)\n",
    "\n",
    "test_model(vgg16, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40cec464-7bfb-46c9-a46c-5e3bcc5d6462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 32.90302093336899\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 2, Loss: 17.260313873916317\n",
      "Accuracy on validation set: 56.25%\n",
      "Epoch 3, Loss: 6.175108407721198\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 4, Loss: 4.192462797728053\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 5, Loss: 1.3568031049213527\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 6, Loss: 1.3832852904416302\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 7, Loss: 1.1531078449787537\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 8, Loss: 0.9464398456131754\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 9, Loss: 0.7738478930632761\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 10, Loss: 1.066498408288312\n",
      "Accuracy on validation set: 50.0%\n",
      "Accuracy on the test set: 62.50%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize the images to 256x256 pixels\n",
    "    transforms.ToTensor(),          # Convert the images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the tensors\n",
    "])\n",
    "\n",
    "# Create datasets for training, validation, and test sets\n",
    "train_dataset = datasets.ImageFolder(root='train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root='val', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root='test', transform=transform)\n",
    "\n",
    "# Create dataloaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# These loaders can now be used to iterate over the respective datasets\n",
    "# Example usage:\n",
    "# for images, labels in train_loader:\n",
    "#     # Training process here\n",
    "\n",
    "\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze the feature layers (optional, if you want to fine-tune only the classifier)\n",
    "for param in vgg16.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the classifier for binary classification\n",
    "vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, 2)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vgg16.classifier.parameters(), lr=0.01)\n",
    "\n",
    "# Function for training\n",
    "def train_model_vgg16(model, criterion, optimizer, train_loader, val_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}, Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Example of how to call the train_model_vgg16 function\n",
    "# train_model_vgg16(vgg16, criterion, optimizer, train_loader, val_loader, epochs=5)\n",
    "\n",
    "# Train the model\n",
    "train_model(vgg16, criterion, optimizer, train_loader, val_loader, epochs=10)\n",
    "\n",
    "test_model(vgg16, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "891fab57-895f-47a4-b2b1-54218c09990f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.42077006940346745\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 2, Loss: 0.24801131824678363\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 3, Loss: 0.11460055827981906\n",
      "Accuracy on validation set: 93.75%\n",
      "Epoch 4, Loss: 0.20752972310799755\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 5, Loss: 0.15535541203492975\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 6, Loss: 0.13855507197638722\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 7, Loss: 0.1957843880028548\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 8, Loss: 0.11282607988584796\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 9, Loss: 0.0996061168965581\n",
      "Accuracy on validation set: 93.75%\n",
      "Epoch 10, Loss: 0.15546198658373786\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 11, Loss: 0.13924415714156282\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 12, Loss: 0.11282825901011453\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 13, Loss: 0.060005903128812814\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 14, Loss: 0.0817642373010635\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 15, Loss: 0.05865890406859767\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 16, Loss: 0.09491730810445463\n",
      "Accuracy on validation set: 93.75%\n",
      "Epoch 17, Loss: 0.059078143917081515\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 18, Loss: 0.07865882676309169\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 19, Loss: 0.11870687953602728\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 20, Loss: 0.11066673978814603\n",
      "Accuracy on validation set: 100.0%\n",
      "Accuracy on the test set: 80.77%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize the images to 256x256 pixels\n",
    "    transforms.ToTensor(),          # Convert the images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the tensors\n",
    "])\n",
    "\n",
    "# Create datasets for training, validation, and test sets\n",
    "train_dataset = datasets.ImageFolder(root='train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root='val', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root='test', transform=transform)\n",
    "\n",
    "# Create dataloaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# These loaders can now be used to iterate over the respective datasets\n",
    "# Example usage:\n",
    "# for images, labels in train_loader:\n",
    "#     # Training process here\n",
    "\n",
    "\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze the feature layers (optional, if you want to fine-tune only the classifier)\n",
    "for param in vgg16.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the classifier for binary classification\n",
    "vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, 2)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vgg16.classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Function for training\n",
    "def train_model_vgg16(model, criterion, optimizer, train_loader, val_loader, epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}, Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Example of how to call the train_model_vgg16 function\n",
    "# train_model_vgg16(vgg16, criterion, optimizer, train_loader, val_loader, epochs=5)\n",
    "\n",
    "# Train the model\n",
    "train_model(vgg16, criterion, optimizer, train_loader, val_loader, epochs=20)\n",
    "\n",
    "test_model(vgg16, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2386adb4-1470-40a0-b4a2-11a329ac2538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 32.5406132304047\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 2, Loss: 30.30872671855785\n",
      "Accuracy on validation set: 93.75%\n",
      "Epoch 3, Loss: 5.380412235581802\n",
      "Accuracy on validation set: 56.25%\n",
      "Epoch 4, Loss: 4.822166871348042\n",
      "Accuracy on validation set: 56.25%\n",
      "Epoch 5, Loss: 4.303916514651176\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 6, Loss: 2.595800938690367\n",
      "Accuracy on validation set: 81.25%\n",
      "Epoch 7, Loss: 0.6025137799831987\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 8, Loss: 1.1074868936114517\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 9, Loss: 1.5964899124360523\n",
      "Accuracy on validation set: 81.25%\n",
      "Epoch 10, Loss: 0.9556416611364282\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 11, Loss: 0.562663707082257\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 12, Loss: 2.375122435619495\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 13, Loss: 1.242595137262637\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 14, Loss: 1.1010987211955836\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 15, Loss: 0.7718740399995464\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 16, Loss: 1.1004903932656247\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 17, Loss: 1.1915968555979934\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 18, Loss: 1.0132988464612902\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 19, Loss: 1.7110725188730684\n",
      "Accuracy on validation set: 50.0%\n",
      "Epoch 20, Loss: 0.7506351201263674\n",
      "Accuracy on validation set: 50.0%\n",
      "Accuracy on the test set: 62.50%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize the images to 256x256 pixels\n",
    "    transforms.ToTensor(),          # Convert the images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the tensors\n",
    "])\n",
    "\n",
    "# Create datasets for training, validation, and test sets\n",
    "train_dataset = datasets.ImageFolder(root='train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root='val', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root='test', transform=transform)\n",
    "\n",
    "# Create dataloaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# These loaders can now be used to iterate over the respective datasets\n",
    "# Example usage:\n",
    "# for images, labels in train_loader:\n",
    "#     # Training process here\n",
    "\n",
    "\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze the feature layers (optional, if you want to fine-tune only the classifier)\n",
    "for param in vgg16.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the classifier for binary classification\n",
    "vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, 2)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vgg16.classifier.parameters(), lr=0.01)\n",
    "\n",
    "# Function for training\n",
    "def train_model_vgg16(model, criterion, optimizer, train_loader, val_loader, epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}, Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Example of how to call the train_model_vgg16 function\n",
    "# train_model_vgg16(vgg16, criterion, optimizer, train_loader, val_loader, epochs=5)\n",
    "\n",
    "# Train the model\n",
    "train_model(vgg16, criterion, optimizer, train_loader, val_loader, epochs=20)\n",
    "\n",
    "test_model(vgg16, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a05e00c8-cde2-4453-b7fe-664c8b417d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.45432769076718715\n",
      "Accuracy on training set: 94.97699386503068%\n",
      "Accuracy on validation set: 81.25%\n",
      "Epoch 2, Loss: 0.29106203819539445\n",
      "Accuracy on training set: 97.27760736196319%\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 3, Loss: 0.18277704269190295\n",
      "Accuracy on training set: 97.58435582822086%\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 4, Loss: 0.147196002517744\n",
      "Accuracy on training set: 96.68328220858896%\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 5, Loss: 0.176970233942562\n",
      "Accuracy on training set: 96.0122699386503%\n",
      "Accuracy on validation set: 93.75%\n",
      "Epoch 6, Loss: 0.20694352365910687\n",
      "Accuracy on training set: 98.1786809815951%\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 7, Loss: 0.14029944181835788\n",
      "Accuracy on training set: 98.1978527607362%\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 8, Loss: 0.1326084211142841\n",
      "Accuracy on training set: 97.45015337423312%\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 9, Loss: 0.12776223714810137\n",
      "Accuracy on training set: 98.33205521472392%\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 10, Loss: 0.17819651251765614\n",
      "Accuracy on training set: 98.2170245398773%\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 11, Loss: 0.12734528717729893\n",
      "Accuracy on training set: 98.60046012269939%\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 12, Loss: 0.1199666997759733\n",
      "Accuracy on training set: 98.33205521472392%\n",
      "Accuracy on validation set: 93.75%\n",
      "Epoch 13, Loss: 0.10188219968627345\n",
      "Accuracy on training set: 98.33205521472392%\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 14, Loss: 0.10149454459422297\n",
      "Accuracy on training set: 98.52377300613497%\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 15, Loss: 0.09064162152067547\n",
      "Accuracy on training set: 98.60046012269939%\n",
      "Accuracy on validation set: 93.75%\n",
      "Epoch 16, Loss: 0.12837098073246656\n",
      "Accuracy on training set: 98.58128834355828%\n",
      "Accuracy on validation set: 87.5%\n",
      "Epoch 17, Loss: 0.11605442565445505\n",
      "Accuracy on training set: 97.7760736196319%\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 18, Loss: 0.09854739043323117\n",
      "Accuracy on training set: 98.2170245398773%\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 19, Loss: 0.1242036194408611\n",
      "Accuracy on training set: 98.14033742331289%\n",
      "Accuracy on validation set: 100.0%\n",
      "Epoch 20, Loss: 0.1816087001386489\n",
      "Accuracy on training set: 98.6771472392638%\n",
      "Accuracy on validation set: 93.75%\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the images horizontally\n",
    "    transforms.RandomRotation(10),      # Randomly rotate the images by 10 degrees\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation and Test transform does not need augmentation, only resizing and normalization\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Apply the updated transforms to datasets\n",
    "train_dataset = datasets.ImageFolder(root='train', transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder(root='val', transform=val_test_transform)\n",
    "test_dataset = datasets.ImageFolder(root='test', transform=val_test_transform)\n",
    "\n",
    "# Create dataloaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze the feature layers (optional, if you want to fine-tune only the classifier)\n",
    "for param in vgg16.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the classifier for binary classification\n",
    "vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, 2)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vgg16.classifier.parameters(), lr=0.001)\n",
    "epochs = 20\n",
    "# Function for training\n",
    "def train_model_vgg16(model, criterion, optimizer, train_loader, val_loader, epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}, Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}, Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# Example of how to call the train_model_vgg16 function\n",
    "# train_model_vgg16(vgg16, criterion, optimizer, train_loader, val_loader, epochs=5)\n",
    "\n",
    "# Train the model\n",
    "train_model(vgg16, criterion, optimizer, train_loader, val_loader, epochs)\n",
    "\n",
    "#test_model(vgg16, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706646ae-a450-4587-a867-e50b145acfd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
